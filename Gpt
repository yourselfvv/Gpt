import numpy as np
from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import random, wikipedia
f = list(open('kind.txt', 'r', encoding='UTF-8'))
f1 = open('data.txt', 'r', encoding='UTF-8')
kinds = []
kinds_OX = []
questions = []
answers = []
for _ in f1:
    questions.append(_[:_.find(',')])
    answers.append(_[_.find(',')+1:-2])
    kinds.append(f"{_[:_.find(',')]}")
    kinds_OX.append('X')
    kinds.append(f"{str(random.choice(f))[:-1]}")
    kinds_OX.append('O')

tokenizer = Tokenizer()
tokenizer.fit_on_texts(kinds)
total_words = len(tokenizer.word_index) + 1
input_sequences = tokenizer.texts_to_sequences(kinds)
input_sequences = pad_sequences(input_sequences)
model = keras.Sequential()
try:
    model.predict("modelvk")
except:
    model.add(keras.layers.Embedding(total_words, 100, input_length=len(input_sequences[0])))
    model.add(keras.layers.LSTM(100))
    model.add(keras.layers.Dense(total_words, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam')

    model.fit(input_sequences, kinds_OX, epochs=20)
    model.save("modelvk")

tokenizer = Tokenizer()
tokenizer.fit_on_texts(questions)
input_sequences = tokenizer.texts_to_sequences(questions)
input_sequences = pad_sequences(input_sequences)
model1 = keras.Sequential()
try:
    model1.predict("modelv1")
except:
    model1.add(keras.layers.Embedding(total_words, 100, input_length=len(input_sequences[0])))
    model1.add(keras.layers.LSTM(100))
    model1.add(keras.layers.Dense(total_words, activation='softmax'))

    model1.compile(loss='categorical_crossentropy', optimizer='adam')

    model1.fit(input_sequences, answers, epochs=20)
    model1.save("modelv1")
while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        break
    input_sequence = tokenizer.texts_to_sequences([user_input])
    input_sequence = pad_sequences(input_sequence, maxlen=len(input_sequences[0]))
    predicted_word_index = np.argmax(model.predict(input_sequence), axis=-1)
    response = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index]
    if " ".join(response) == "O":
        qword = input('단어: ')
        print(wikipedia.page(qword).text)
    else:
        predicted_word_index = np.argmax(model1.predict(input_sequence), axis=-1)
        response = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index]
        print("".join(response))
